import chainlit as cl
from transformers import AutoModelForCausalLM, AutoProcessor, GenerationConfig
from transformers.utils.logging import set_verbosity_error
from PIL import Image
from pathlib import Path
import csv
import torch
import os
import time
import json
import re

os.environ["TF_CPP_MIN_LOG_LEVEL"] = "2"
os.environ["TOKENIZERS_PARALLELISM"] = "false"
set_verbosity_error()

# å•Ÿç”¨ TF32 / cuDNN æœ€ä½³åŒ–
torch.backends.cuda.matmul.allow_tf32 = True
torch.backends.cudnn.allow_tf32 = True
torch.backends.cudnn.benchmark = True
torch.set_float32_matmul_precision("high")

# é è¨­æç¤ºï¼ˆç•¶ä½¿ç”¨è€…æ²’æœ‰è¼¸å…¥ä»»ä½•æ–‡å­—æ™‚ä½¿ç”¨ï¼‰
DEFAULT_PROMPT = """
You are a forest aerial-imagery analysis assistant. Please review the provided aerial photo and complete the following tasks in **two stages**:

## Execution order (do not skip)
1) Run Stage 1. If NOT forest aerial â‡’ output Stage 1 result and STOP.
2) Only if Stage 1 confirms forest aerial â‡’ run Stage 2.

## Stage 1: Scenario check
- If the image is NOT an outdoor aerial photo of a forest (including indoor photos, close-up objects, animals, portraits, city, desert, ocean, farmland, etc.), you MUST classify it as:
  * "stage": "Stage 1"
  * "priority": "No related event"
  * "event_name": "Not Related to Forest Aerial Analysis"
- Only if it is clearly an outdoor aerial photo of a forest, continue to Stage 2.

## Stage 2: Forest event detection (conservative)
- **Wildfire (p0)** â€” Use only if there are **clear and obvious** signs **within the forested area**, not clouds/fog/industrial plumes:
  - At least **one** must be clearly visible **and** localized: **smoke plume/column**, **distinct fire line**, **visible flame**, **burned/scorched/charred** band.
  - "reasons" must include a **spatial reference** (e.g., â€œtop-left quadrantâ€, â€œsouth of the riverâ€).
  - If chosen, set:
    - "priority": "p0"
    - "event_name": "Forest Wildfire"

- **Landslide / Debris Flow (p0)** â€” Use only if there are **clear and obvious** geomorphic signatures in forested terrain (do not confuse with logging clearings or natural river bars):
  - Examples of qualifying evidence (at least **one** clearly visible **and** localized): **fresh slope-failure scar** with bare soil/rock; **debris-flow runout channel** or **leveed tongue**; **sediment/debris fan** at a valley mouth; **boulder/mud deposits** along a channel; **channel blockage/damming** or abrupt avulsion; **turbid sediment plume** downstream of a failure.
  - "reasons" must include a **spatial reference**.
  - If chosen, set:
    - "priority": "p0"
    - "event_name": "Landslide / Debris Flow"

- **Deforestation / Illegal Logging (p1)** â€” Use only if **both** conditions hold (and are not negated/uncertain):
  1) **Heavy machinery present** (e.g., excavator/harvester/logging truck), **AND**
  2) **Large cleared area and/or tree stumps and/or bare soil/earthworks** consistent with tree removal.
  - "reasons" must include a **spatial reference**.
  - If one of the two conditions is missing or unclear, **do not** output p1; use "No event detected".
  - If chosen, set:
    - "priority": "p1"
    - "event_name": "Deforestation / Illegal Logging"

- **No event detected** â€” If it is a forest aerial photo but the above event criteria are **not fully satisfied**, return:
  - If chosen, set:
    - "priority": "No event detected"
    - "event_name": "Normal Forest Condition"

---
## Output rules (HARD CONSTRAINTS):
- You MUST always output a field called "stage":
  * If you stopped at Stage 1 (image is not a forest aerial photo), set "stage": "Stage 1".
  * If you continued to Stage 2, set "stage": "Stage 2".

Return one JSON object with exactly the keys: priority, event_name, reasons.

Now, output the result for the given image strictly in the following JSON format (no extra text):

{
"stage": "Stage 1 | Stage 2",
"priority": "p0 | p1 | No event detected | No related event",
"event_name": "<event type in words>",
"reasons": "<specific evidence supporting the assigned priority>"
}
""".strip()

# Test processor and model loading
try:
    print("Loading processor...")
    processor = AutoProcessor.from_pretrained(
        'allenai/Molmo-7B-D-0924',
        trust_remote_code=True,
        use_fast=True,
    )
    print("Processor loaded successfully!")
    
    print("Loading model...")
    model = AutoModelForCausalLM.from_pretrained(
        'allenai/Molmo-7B-D-0924',
        trust_remote_code=True,
        do_sample=False,    # å–æ¶ˆéš¨æ©Ÿå–æ¨£
        torch_dtype=torch.float16,
        device_map={"": 0},   # æŒ‡å®šä¸Ÿåˆ°ç¬¬ 0 å¼µ GPU
    )
    print("Model loaded successfully!")
    
except Exception as e:
    print(f"Error loading model/processor: {e}")
    raise

def _to_model_device_and_dtype(x):
    """
    éè¿´è™•ç†:
    - æµ®é» torch.Tensor -> æ¬åˆ° model.device + è½‰ç‚º model.dtype + unsqueeze(0)
    - æ•´æ•¸/å¸ƒæ— torch.Tensor -> åªæ¬åˆ° model.device + unsqueeze(0)
    - list/tuple -> é€ä¸€è™•ç†ä¸¦ä¿ç•™å‹åˆ¥
    - dict -> é€ä¸€è™•ç†ä¸¦ä¿ç•™éµå€¼
    - å…¶ä»–å‹åˆ¥ -> åŸæ¨£è¿”å›
    """
    if torch.is_tensor(x):
        if x.dtype in (torch.float16, torch.float32, torch.float64, torch.bfloat16):
            return x.to(device=model.device, dtype=getattr(model, "dtype", torch.float16)).unsqueeze(0)
        else:
            return x.to(device=model.device).unsqueeze(0)

    if isinstance(x, (list, tuple)):
        proc = [ _to_model_device_and_dtype(v) for v in x ]
        return type(x)(proc)

    if isinstance(x, dict):
        return { k: _to_model_device_and_dtype(v) for k, v in x.items() }

    return x

# --- å•Ÿå‹•æ™‚åšä¸€æ¬¡é ç†±---
def _warmup_once():
    try:
        img = Image.new("RGB", (64, 64), (0, 128, 0))
        dummy = processor.process(images=[img], text="Warm up.")
        dummy = _to_model_device_and_dtype(dummy)  # åªåšä¸€æ¬¡ã€éè¿´è™•ç†
        with torch.inference_mode():
            _ = model.generate_from_batch(
                dummy,
                GenerationConfig(max_new_tokens=8, do_sample=False),
                tokenizer=processor.tokenizer
            )
        print("Warmup done.")
    except Exception as e:
        print("Warmup skipped:", e)

_warmup_once()

@cl.on_chat_start
async def start():
    await cl.Message("æ­¡è¿ä½¿ç”¨åœ–åƒåˆ†ææ‡‰ç”¨!è«‹ä¸Šå‚³ä¸€å¼µåœ–ç‰‡ï¼Œç„¶å¾Œè¼¸å…¥æ‚¨çš„å•é¡Œæˆ–æè¿°è¦æ±‚ã€‚").send()

@cl.on_message
async def main(message: cl.Message):
    if not message.elements:
        await cl.Message("è«‹å…ˆä¸Šå‚³ä¸€å¼µåœ–ç‰‡ï¼Œç„¶å¾Œå†è¼¸å…¥ä½ çš„å•é¡Œã€‚").send()
        return
    
    image = message.elements[0]
    if not image.mime.startswith("image"):
        await cl.Message("è«‹ä¸Šå‚³ä¸€å€‹æœ‰æ•ˆçš„åœ–ç‰‡æ–‡ä»¶ã€‚").send()
        return
    
    user_prompt = message.content.strip() if message.content else DEFAULT_PROMPT
    
    await process_image(image.path, user_prompt)
    
def _cuda_sync_if_available():
    # åœ¨é‡æ¸¬ GPU æ™‚é–“é»åšåŒæ­¥ï¼Œé¿å…éåŒæ­¥é€ æˆæ™‚é–“å¤±çœŸ
    if torch.cuda.is_available():
        try:
            torch.cuda.synchronize()
        except Exception:
            pass

# å¾Œè™•ç†å‡½æ•¸ï¼šè‹¥ stage ç‚º Stage2 ä¸” reasons å« "possible"ï¼ˆä¸åˆ†å¤§å°å¯«ï¼‰ï¼Œå¼·åˆ¶æ”¹ç‚º No event detected
def postprocess_possible_downgrade(text: str, replace_reason: bool = False) -> str:
    """
    å…¥åƒ:
      text: æ¨¡å‹åŸå§‹è¼¸å‡ºï¼ˆé æœŸç‚º JSON å­—ä¸²ï¼›è‹¥ä¸æ˜¯ JSONï¼Œå°‡åŸæ¨£è¿”å›ï¼‰
      replace_reason: è‹¥ç‚º Trueï¼ŒæœƒæŠŠ reasons æ”¹æˆå›ºå®šå¥å­

    å›å‚³:
      å¾Œè™•ç†å¾Œçš„ JSON å­—ä¸²ï¼ˆè‹¥é JSON æˆ–è§£æå¤±æ•—ï¼Œå°±åŸæ¨£è¿”å›ï¼‰
    """
    # å˜—è©¦è§£æç‚º JSONï¼›è‹¥ä¸æ˜¯ç´” JSONï¼Œå˜—è©¦æ“·å–ç¬¬ä¸€å€‹ {...}
    try:
        data = json.loads(text)
    except json.JSONDecodeError:
        m = re.search(r"\{[\s\S]*\}", text)
        if not m:
            return text
        try:
            data = json.loads(m.group(0))
        except Exception:
            return text

    # åƒ…åœ¨ stage ç‚º Stage 2/Stage2 æ™‚æ‰æª¢æŸ¥ possible
    stage_str = str(data.get("stage", ""))
    is_stage2 = re.search(r"\bstage\s*2\b", stage_str, flags=re.I) is not None

    reasons = str(data.get("reasons", ""))
    has_possible = re.search(r"\bpossible\b", reasons, flags=re.I) is not None

    if is_stage2 and has_possible:
        data["stage"] = "Stage 2"
        data["priority"] = "No event detected"
        data["event_name"] = "Normal Forest Condition"
        if replace_reason:
            data["reasons"] = "No event detected in the aerial image"

    # ç¢ºä¿å¿…è¦éµå­˜åœ¨ï¼ˆé¿å…ä¸‹æ¸¸å–å€¼å ±éŒ¯ï¼‰
    for k in ("stage", "priority", "event_name", "reasons"):
        data.setdefault(k, "")

    return json.dumps(data, ensure_ascii=False)

async def batch_process_images(
    dir_path="images",
    prompt=DEFAULT_PROMPT,
    recursive=True,
):
    exts = {".jpg", ".jpeg", ".png", ".bmp", ".webp", ".tif", ".tiff", ".gif"}
    base = Path(dir_path)
    if not base.exists():
        await cl.Message(f"`{dir_path}` ä¸å­˜åœ¨ã€‚").send()
        return

    paths = (base.rglob("*") if recursive else base.glob("*"))
    paths = [p for p in paths if p.suffix.lower() in exts and p.is_file()]
    paths.sort()

    if not paths:
        await cl.Message(f"`{dir_path}` å…§æ²’æœ‰å¯è™•ç†çš„åœ–ç‰‡ã€‚").send()
        return

    await cl.Message(f"ğŸ” ç™¼ç¾ {len(paths)} å¼µåœ–ç‰‡ï¼Œé–‹å§‹æ‰¹æ¬¡æ¨è«–â€¦â€¦").send()

    for p in paths:
        try:
            # include_filename=Trueï¼šè¨Šæ¯æœƒæŠŠæª”åå’Œ JSON ä¸€èµ·é¡¯ç¤º
            # send_perf=Falseï¼šæ‰¹æ¬¡æ™‚ä¸åˆ·æ•ˆèƒ½æ‘˜è¦ï¼Œé¿å…æ´—ç‰ˆ
            text = await process_image(str(p), prompt, include_filename=True, send_perf=False)
            out = text if (text and text.strip()) else "<EMPTY>"
        except Exception as e:
            out = f"<ERROR: {e}>"

        # console ä¸€è¡Œè¼¸å‡ºï¼šæª”å \t JSON
        print(f"{p.name}\t{out}")
        
@cl.on_chat_start
async def start():
    await cl.Message("æ­¡è¿ä½¿ç”¨åœ–åƒåˆ†ææ‡‰ç”¨ï¼å·²è‡ªå‹•æƒæ `images/` ä¸¦åŸ·è¡Œæƒ…å¢ƒçœŸæ¸¬ã€‚ä½ ä¹Ÿå¯ä»¥å†ä¸Šå‚³å–®å¼µåœ–ç‰‡åšäº’å‹•åˆ†æã€‚").send()
    await batch_process_images(dir_path="images", prompt=DEFAULT_PROMPT, recursive=True)
        
async def process_image(image_path, user_prompt, include_filename=False, send_perf=True):
    try:
        overall_s = time.perf_counter()
        print("=" * 50)
        print(f"é–‹å§‹è™•ç†åœ–ç‰‡: {image_path}")
        print(f"ç”¨æˆ¶æç¤º: {user_prompt}")
        
        # 1) è®€åœ–
        t_img_s = time.perf_counter()
        try:
            image = Image.open(image_path).convert("RGB") 
            image.thumbnail((1536, 1536), Image.Resampling.LANCZOS) # é å…ˆç¸®åœ–ï¼ˆé¿å…è¶…å¤§åœ–åƒç®—åŠ›èˆ‡è¨˜æ†¶é«”ï¼‰
            print(f"åœ–ç‰‡è¼‰å…¥æˆåŠŸ: {image.size}, æ¨¡å¼: {image.mode}")
        except Exception as e:
            await cl.Message(f"ç„¡æ³•è¼‰å…¥åœ–ç‰‡: {str(e)}").send()
            return
        
        t_img_e = time.perf_counter()
        
        # 2) å‰è™•ç†ï¼ˆprocessor.processï¼‰
        t_proc_s = time.perf_counter()
        try:
            print("é–‹å§‹è™•ç†è¼¸å…¥...")
            inputs = processor.process(
                images=[image],
                text=user_prompt
            )
            print(f"è™•ç†å™¨è¿”å›é¡å‹: {type(inputs)}")
            if inputs:
                print(f"è™•ç†å™¨è¿”å›çš„éµ: {list(inputs.keys())}")
                for key, value in inputs.items():
                    if value is not None:
                        print(f"{key}: {type(value)}, å½¢ç‹€: {getattr(value, 'shape', 'N/A')}")
                    else:
                        print(f"{key}: None")
            else:
                print("è™•ç†å™¨è¿”å› None!")
                await cl.Message("è™•ç†å™¨è¿”å›ç©ºçµæœï¼Œè«‹æª¢æŸ¥æ¨¡å‹å®‰è£ã€‚").send()
                return
                
        except Exception as e:
            print(f"è™•ç†å™¨éŒ¯èª¤: {e}")
            await cl.Message(f"è™•ç†å™¨éŒ¯èª¤: {str(e)}").send()
            return
        t_proc_e = time.perf_counter()
        
        # Check essential inputs
        if 'input_ids' not in inputs or inputs['input_ids'] is None:
            await cl.Message("è™•ç†å™¨æ²’æœ‰è¿”å›æœ‰æ•ˆçš„ input_ids").send()
            return
        
        # 3) æ¬åˆ°è£ç½®ä¸¦åŠ  batch ç¶­åº¦
        t_move_s = time.perf_counter()
        try:
            print("ç§»å‹•åˆ°è¨­å‚™ä¸¦æ·»åŠ æ‰¹æ¬¡ç¶­åº¦...")
            inputs = _to_model_device_and_dtype(inputs)  # ä¸€æ¬¡åˆ°ä½ï¼Œçµæ§‹ä¿ç•™
            #ï¼ˆå¯é¸ï¼‰å°å‡ºé—œéµå¼µé‡çš„ dtype / shape ä»¥ç¢ºèª
            if torch.is_tensor(inputs.get("input_ids", None)):
                print(f"input_ids: {inputs['input_ids'].shape}, {inputs['input_ids'].dtype}, {inputs['input_ids'].device}")
        except Exception as e:
            print(f"è¨­å‚™ç§»å‹•éŒ¯èª¤: {e}")
            await cl.Message(f"è¨­å‚™ç§»å‹•éŒ¯èª¤: {str(e)}").send()
            return
        t_move_e = time.perf_counter()
        
        # 4) ç”Ÿæˆï¼ˆæ¨è«–ï¼‰
        _cuda_sync_if_available()
        t_gen_s = time.perf_counter()
        try:
            print("é–‹å§‹ç”Ÿæˆ...")
            output = model.generate_from_batch(
                inputs,
                GenerationConfig(max_new_tokens=120, stop_strings=["\n}", "\n}\n", "<|endoftext|>"], do_sample=False),
                tokenizer=processor.tokenizer
            )
            print(f"ç”Ÿæˆå®Œæˆï¼Œè¼¸å‡ºå½¢ç‹€: {output.shape}")
            
        except Exception as e:
            print(f"ç”ŸæˆéŒ¯èª¤: {e}")
            await cl.Message(f"ç”ŸæˆéŒ¯èª¤: {str(e)}").send()
            return
        _cuda_sync_if_available()
        t_gen_e = time.perf_counter()
        
        # 5) è§£ç¢¼
        t_dec_s = time.perf_counter()
        try:
            # Get generated tokens and decode them to text
            input_length = inputs['input_ids'].size(1)
            total_len = output.shape[1]
            new_tokens = max(0, total_len - input_length)
            generated_tokens = output[0, input_length:]
            generated_text = processor.tokenizer.decode(generated_tokens, skip_special_tokens=True)

            # å¾Œè™•ç†
            generated_text = postprocess_possible_downgrade(generated_text, replace_reason=True)
            
            print(f"ç”Ÿæˆçš„æ–‡æœ¬: {generated_text}")
            
            # === é€™è£¡ä¾éœ€æ±‚æŠŠæª”åä¸€èµ·è¼¸å‡ºåˆ°è¨Šæ¯ ===
            if generated_text.strip():
                if include_filename:
                    content = f"**{os.path.basename(image_path)}**\n```json\n{generated_text.strip()}\n```"
                else:
                    content = generated_text.strip()
                await cl.Message(content=content).send()
            else:
                await cl.Message("æ¨¡å‹æ²’æœ‰ç”Ÿæˆä»»ä½•æ–‡æœ¬ï¼Œè«‹å˜—è©¦å…¶ä»–å•é¡Œã€‚").send()
                
        except Exception as e:
            print(f"è§£ç¢¼éŒ¯èª¤: {e}")
            await cl.Message(f"è§£ç¢¼éŒ¯èª¤: {str(e)}").send()
            return
        t_dec_e = time.perf_counter()
        overall_e = time.perf_counter()
        
        # æ¨è«–æ•ˆèƒ½çµ±è¨ˆ
        t_img = (t_img_e - t_img_s) * 1000.0
        t_proc = (t_proc_e - t_proc_s) * 1000.0
        t_move = (t_move_e - t_move_s) * 1000.0
        t_gen = (t_gen_e - t_gen_s)          # ç§’
        t_dec = (t_dec_e - t_dec_s) * 1000.0
        t_total = (overall_e - overall_s)    # ç§’
        toks_per_sec = (new_tokens / t_gen) if t_gen > 0 else float('nan')
        
        # å†é€å‡ºæ•ˆèƒ½æ‘˜è¦ï¼ˆç¨ç«‹è¨Šæ¯ï¼Œä¸å½±éŸ¿ä¸Šé¢çš„ JSONï¼‰
        perf_msg = (
            "â±ï¸ **æ¨è«–æ•ˆèƒ½æ‘˜è¦**\n"
            f"- è®€åœ–: {t_img:.1f} ms\n"
            f"- å‰è™•ç†(processor): {t_proc:.1f} ms\n"
            f"- æ¬åˆ°è£ç½®: {t_move:.1f} ms\n"
            f"- ç”Ÿæˆ(æ¨è«–): {t_gen*1000.0:.1f} ms\n"
            f"- è§£ç¢¼: {t_dec:.1f} ms\n"
            f"- **ç«¯åˆ°ç«¯ç¸½æ™‚é–“**: {t_total:.3f} s\n"
            f"- ç”¢ç”Ÿçš„æ–° tokens: {new_tokens}\n"
            f"- **ååé‡**: {toks_per_sec:.2f} tokens/s\n"
        )
        
        if send_perf:
            await cl.Message(content=perf_msg).send()

        return generated_text.strip()
            
    except Exception as e:
        import traceback
        print(f"ç¸½é«”éŒ¯èª¤: {e}")
        print(f"å®Œæ•´å †æ£§:\n{traceback.format_exc()}")
        await cl.Message(f"è™•ç†åœ–ç‰‡æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}").send()

if __name__ == "__main__":
    cl.run()
